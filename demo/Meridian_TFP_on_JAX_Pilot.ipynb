{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "initial_buttons"
      },
      "source": [
        "\u003ctable class=\"tfo-notebook-buttons\" align=\"left\"\u003e\n",
        "  \u003ctd\u003e\n",
        "    \u003ca target=\"_blank\" href=\"https://colab.research.google.com/github/google/meridian/blob/main/demo/Meridian_TFP_on_JAX_Pilot.ipynb\"\u003e\u003cimg src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" /\u003eRun in Google Colab\u003c/a\u003e\n",
        "  \u003c/td\u003e\n",
        "  \u003ctd\u003e\n",
        "    \u003ca target=\"_blank\" href=\"https://github.com/google/meridian/blob/main/demo/Meridian_TFP_on_JAX_Pilot.ipynb\"\u003e\u003cimg src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" /\u003eView source on GitHub\u003c/a\u003e\n",
        "  \u003c/td\u003e\n",
        "\u003c/table\u003e"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "title_cell"
      },
      "source": [
        "# **Meridian TFP-on-JAX Pilot**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "goal_cell"
      },
      "source": [
        "This notebook demonstrates how to implement and use JAX/TFP-based versions of the Adstock and Hill functions, which are core components of Media Mix Models (MMMs). \n",
        "\n",
        "The Adstock function models the lagged carryover effects of media advertising, while the Hill function models the diminishing returns or saturation effect.\n",
        "\n",
        "We will use the standard sample dataset provided by the Meridian library (`google-meridian`) to illustrate these concepts in a TFP-on-JAX context."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "install_markdown"
      },
      "source": [
        "## Step 0: Install necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_code"
      },
      "outputs": [],
      "source": [
        "# Install JAX, TFP nightly, and Meridian for data loading\n",
        "!pip install -qU jax jaxlib\n",
        "!pip install -qU tfp-nightly\n",
        "!pip install -qU google-meridian"
      ]
    },
     {
      "cell_type": "markdown",
      "metadata": {
        "id": "import_markdown"
      },
      "source": [
        "## Step 1: Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "import_code"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow_probability as tfp\n",
        "tfp = tfp.experimental.substrates.jax\n",
        "tfd = tfp.distributions\n",
        "\n",
        "# Import Meridian's data loading utility\n",
        "from meridian.data import load"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "load_data_markdown"
      },
      "source": [
        "## Step 2: Load the data"
      ]
    },
      {
      "cell_type": "markdown",
      "metadata": {
        "id": "map_columns_markdown"
      },
      "source": [
        "Define mappings from column names to variable types and channel names."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "map_columns_code"
      },
      "outputs": [],
      "source": [
        "# These mappings are needed by the Meridian data loader\n",
        "coord_to_columns = load.CoordToColumns(\n",
        "    time='time',\n",
        "    geo='geo',\n",
        "    controls=['GQV', 'Competitor_Sales'],\n",
        "    population='population',\n",
        "    kpi='conversions',\n",
        "    revenue_per_kpi='revenue_per_conversion',\n",
        "    media=[\n",
        "        'Channel0_impression',\n",
        "        'Channel1_impression',\n",
        "        'Channel2_impression',\n",
        "        'Channel3_impression',\n",
        "        'Channel4_impression',\n",
        "    ],\n",
        "    media_spend=[\n",
        "        'Channel0_spend',\n",
        "        'Channel1_spend',\n",
        "        'Channel2_spend',\n",
        "        'Channel3_spend',\n",
        "        'Channel4_spend',\n",
        "    ],\n",
        "    organic_media=['Organic_channel0_impression'],\n",
        "    non_media_treatments=['Promo'],\n",
        ")\n",
        "\n",
        "correct_media_to_channel = {\n",
        "    'Channel0_impression': 'Channel_0',\n",
        "    'Channel1_impression': 'Channel_1',\n",
        "    'Channel2_impression': 'Channel_2',\n",
        "    'Channel3_impression': 'Channel_3',\n",
        "    'Channel4_impression': 'Channel_4',\n",
        "}\n",
        "correct_media_spend_to_channel = {\n",
        "    'Channel0_spend': 'Channel_0',\n",
        "    'Channel1_spend': 'Channel_1',\n",
        "    'Channel2_spend': 'Channel_2',\n",
        "    'Channel3_spend': 'Channel_3',\n",
        "    'Channel4_spend': 'Channel_4',\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "load_data_code"
      },
      "outputs": [],
      "source": [
        "# Load the sample data using Meridian's CsvDataLoader\n",
        "loader = load.CsvDataLoader(\n",
        "    csv_path=\"https://raw.githubusercontent.com/google/meridian/refs/heads/main/meridian/data/simulated_data/csv/geo_all_channels.csv\",\n",
        "    kpi_type='non_revenue',\n",
        "    coord_to_columns=coord_to_columns,\n",
        "    media_to_channel=correct_media_to_channel,\n",
        "    media_spend_to_channel=correct_media_spend_to_channel,\n",
        ")\n",
        "data = loader.load()\n",
        "\n",
        "# Display the structure of the loaded data (optional)\n",
        "# print(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "extract_media_data"
      },
      "outputs": [],
      "source": [
        "# Extract media impression data as a JAX array\n",
        "# The loaded 'data' is an xarray Dataset. We access the 'media' DataArray.\n",
        "media_data_jax = jnp.asarray(data['media'].values)\n",
        "\n",
        "# Print the shape (expected: Time x Geo x Media Channel)\n",
        "print(\"Shape of media data (Time x Geo x Media Channel):\", media_data_jax.shape)"
       ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adstock_markdown"
      },
      "source": [
        "## Step 3: Define Adstock Function (TFP-on-JAX)\n",
        "\n",
        "The Adstock transformation models the carryover effect of advertising, where the impact of media spend decays over time. We implement a geometric decay adstock using JAX's convolution capabilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "adstock_code_placeholder"
      },
      "outputs": [],
      "source": [
        "def adstock_jax(media, alpha, max_lag, n_times_output):\n",
        "  \"\"\"Applies geometric adstock transformation using JAX convolution.\n",
        "\n",
        "  Args:\n",
        "    media: JAX array with shape [..., n_geos, n_media_times, n_channels].\n",
        "      Media values over time for different geos and channels.\n",
        "    alpha: JAX array with shape [..., n_channels]. Adstock decay parameter for\n",
        "      each channel.\n",
        "    max_lag: Integer, the maximum duration of the adstock effect.\n",
        "    n_times_output: Integer, the desired number of time points in the output.\n",
        "\n",
        "  Returns:\n",
        "    JAX array with shape [..., n_geos, n_times_output, n_channels] containing\n",
        "    the adstocked media.\n",
        "  \"\"\"\n",
        "  # --- Get input shapes --- \n",
        "  input_shape = jnp.shape(media)\n",
        "  batch_shape = input_shape[:-3]\n",
        "  n_geos = input_shape[-3]\n",
        "  n_media_times = input_shape[-2]\n",
        "  n_channels = input_shape[-1]\n",
        "  alpha_shape = jnp.shape(alpha)\n",
        "  if alpha_shape[-1] != n_channels:\n",
        "      raise ValueError(\n",
        "          f\"Trailing dimension of alpha ({alpha_shape[-1]}) must match \"\n",
        "          f\"trailing dimension of media ({n_channels}).\"\n",
        "      )\n",
        "\n",
        "  # --- Calculate window size and required history --- \n",
        "  window_size = max_lag + 1\n",
        "  required_n_media_times = n_times_output + max_lag\n",
        "\n",
        "  # --- Pad or slice media history --- \n",
        "  if n_media_times < required_n_media_times:\n",
        "    # Pad with zeros at the beginning of the time dimension\n",
        "    padding_amount = required_n_media_times - n_media_times\n",
        "    # Paddings format: [(before_axis0, after_axis0), (before_axis1, after_axis1), ...]\n",
        "    # We need padding only before the time axis (-2)\n",
        "    paddings = [(0, 0)] * (len(input_shape) - 2) + [(padding_amount, 0), (0, 0)]\n",
        "    media_padded = jnp.pad(media, paddings, mode='constant', constant_values=0)\n",
        "  elif n_media_times > required_n_media_times:\n",
        "    # Slice to keep only the most recent required history\n",
        "    start_index = n_media_times - required_n_media_times\n",
        "    media_padded = jax.lax.dynamic_slice_in_dim(\n",
        "        media, start_index, required_n_media_times, axis=-2)\n",
        "  else:\n",
        "    media_padded = media\n",
        "\n",
        "  # --- Calculate adstock weights (convolution kernel) --- \n",
        "  # Range of lags [0, 1, ..., max_lag]\n",
        "  l_range = jnp.arange(window_size, dtype=jnp.float32)\n",
        "  # Geometric decay weights: alpha^lag. Shape: [..., n_channels, window_size]\n",
        "  weights = jnp.power(jnp.expand_dims(alpha, -1), l_range)\n",
        "  # Normalize weights to sum to 1 per channel\n",
        "  normalization = jnp.sum(weights, axis=-1, keepdims=True)\n",
        "  # Avoid division by zero if sum of weights is zero (e.g., alpha=0, max_lag>0)\n",
        "  safe_normalization = jnp.where(normalization == 0, 1.0, normalization)\n",
        "  normalized_weights = jnp.divide(weights, safe_normalization)\n",
        "  # Reverse weights for convolution. Shape: [..., n_channels, window_size]\n",
        "  kernel = normalized_weights[..., ::-1]\n",
        "\n",
        "  # --- Prepare for convolution --- \n",
        "  # Transpose media to [..., n_channels, n_geos, required_n_media_times]\n",
        "  # This groups data by channel first, suitable for vmap over channels.\n",
        "  media_transposed = jnp.moveaxis(media_padded, -1, -3)\n",
        "\n",
        "  # --- Define convolution for a single channel --- \n",
        "  def _convolve_channel(media_ch, kernel_ch):\n",
        "    # media_ch shape: [..., n_geos, required_n_media_times]\n",
        "    # kernel_ch shape: [..., window_size]\n",
        "    \n",
        "    # Reshape media for conv: [..., N=n_geos, W=req_times, C=1]\n",
        "    media_ch_reshaped = jnp.expand_dims(media_ch, axis=-1)\n",
        "    # Reshape kernel for conv: [..., W=window_size, I=1, O=1]\n",
        "    kernel_ch_reshaped = jnp.expand_dims(kernel_ch, axis=(-1, -2))\n",
        "\n",
        "    # Perform 1D convolution along the time axis for each geo\n",
        "    # 'VALID' padding ensures output length is n_times_output\n",
        "    dn = jax.lax.ConvDimensionNumbers(\n",
        "        lhs_spec=(len(media_ch_reshaped.shape) - 3,) + (len(media_ch_reshaped.shape) - 2, len(media_ch_reshaped.shape) - 1),\n",
        "        rhs_spec=(len(kernel_ch_reshaped.shape) - 3,) + (len(kernel_ch_reshaped.shape) - 2, len(kernel_ch_reshaped.shape) - 1),\n",
        "        out_spec=(len(media_ch_reshaped.shape) - 3,) + (len(media_ch_reshaped.shape) - 2, len(media_ch_reshaped.shape) - 1)\n",
        "    ) # Matches NWC, WIO, NWC based on input ranks\n",
        "    \n",
        "    convolved = jax.lax.conv_general_dilated(\n",
        "        lhs=media_ch_reshaped,\n",
        "        rhs=kernel_ch_reshaped,\n",
        "        window_strides=(1,),\n",
        "        padding='VALID',\n",
        "        dimension_numbers=dn,\n",
        "        feature_group_count=1\n",
        "    )\n",
        "    # Output shape: [..., n_geos, n_times_output, 1]\n",
        "    # Squeeze the trailing channel dimension\n",
        "    return jnp.squeeze(convolved, axis=-1)\n",
        "\n",
        "  # --- Apply convolution per channel using vmap --- \n",
        "  # We map over the channel dimension, which is now at axis -3 for media\n",
        "  # and axis -2 for the kernel.\n",
        "  # The output will have the channel dimension at axis -3.\n",
        "  adstocked_media_transposed = jax.vmap(\n",
        "      _convolve_channel, \n",
        "      in_axes=(-3, -2), # Axis indices for media_transposed and kernel\n",
        "      out_axes=-3       # Place the mapped axis back at -3 in the output\n",
        "  )(media_transposed, kernel)\n",
        "  # Output shape: [..., n_channels, n_geos, n_times_output]\n",
        "\n",
        "  # --- Transpose back to original format --- \n",
        "  # Move channel axis from -3 back to -1\n",
        "  adstocked_media = jnp.moveaxis(adstocked_media_transposed, -3, -1)\n",
        "  # Final shape: [..., n_geos, n_times_output, n_channels]\n",
        "\n",
        "  return adstocked_media\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hill_markdown"
      },
      "source": [
        "## Step 4: Define Hill Function (TFP-on-JAX)\n",
        "\n",
        "The Hill function models the saturation effect or diminishing returns of advertising. As media exposure increases, its marginal effect typically decreases. The Hill function captures this non-linear relationship."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hill_code_placeholder"
      },
      "outputs": [],
      "source": [
        "def hill_jax(media, ec, slope):\n",
        "  \"\"\"Applies the Hill transformation using JAX.\n",
        "\n",
        "  Args:\n",
        "    media: JAX array with shape [..., n_geos, n_times, n_channels].\n",
        "      Adstocked media values.\n",
        "    ec: JAX array broadcastable to [..., 1, 1, n_channels].\n",
        "      The concentration parameter controlling the inflection point.\n",
        "      Often has shape [..., n_channels].\n",
        "    slope: JAX array broadcastable to [..., 1, 1, n_channels].\n",
        "      The slope parameter controlling the steepness of the curve.\n",
        "      Often has shape [..., n_channels].\n",
        "\n",
        "  Returns:\n",
        "    JAX array with the same shape as media, containing the transformed values.\n",
        "  \"\"\"\n",
        "  # Validate shapes (optional, good practice)\n",
        "  # media_shape = jnp.shape(media)\n",
        "  # ec_shape = jnp.shape(ec)\n",
        "  # slope_shape = jnp.shape(slope)\n",
        "  # n_channels = media_shape[-1]\n",
        "  # if ec_shape[-1] != n_channels or slope_shape[-1] != n_channels:\n",
        "  #   raise ValueError(\"Trailing dimension of ec and slope must match media.\")\n",
        "\n",
        "  # Ensure ec and slope are broadcastable to media's channel dimension\n",
        "  # JAX handles broadcasting automatically if ec/slope have shape [..., n_channels]\n",
        "\n",
        "  # Calculate term: ec^slope\n",
        "  # JAX will broadcast ec and slope to match the channel dimension\n",
        "  ec_pow_slope = jnp.power(ec, slope)\n",
        "\n",
        "  # Calculate term: media^slope\n",
        "  # JAX will broadcast slope to match media's shape [..., G, T, C]\n",
        "  media_pow_slope = jnp.power(media, slope)\n",
        "\n",
        "  # Calculate denominator: media^slope + ec^slope\n",
        "  denominator = media_pow_slope + ec_pow_slope\n",
        "\n",
        "  # Calculate Hill transformation: media^slope / (media^slope + ec^slope)\n",
        "  # Avoid division by zero if denominator is zero. Output 0 in that case.\n",
        "  # This can happen if media=0 and ec=0.\n",
        "  hill_media = jnp.where(\n",
        "      denominator == 0,\n",
        "      0.0,\n",
        "      jnp.divide(media_pow_slope, denominator)\n",
        "  )\n",
        "\n",
        "  return hill_media"
       ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usage_markdown"
      },
      "source": [
        "## Step 5: Demonstrate Usage"
      ]
    },
    {
       "cell_type": "markdown",
       "metadata": {
         "id": "demo_params_markdown"
        },
       "source": [
        "Now, let's define some example parameters for the Adstock and Hill functions. We'll derive the number of channels and time points from the `media_data_jax` we loaded earlier."
       ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "define_demo_params"
      },
      "outputs": [],
      "source": [
        "# Get dimensions from loaded media data\n",
        "n_geos, n_times, n_channels = media_data_jax.shape\n",
        "print(f\"Detected {n_geos} geos, {n_times} time points, {n_channels} channels.\")\n",
        "\n",
        "# --- Adstock Parameters ---\n",
        "max_lag = 13 # Example max lag (weeks)\n",
        "n_times_output = n_times # Output same number of time points as input\n",
        "# Example alpha values (decay rate) for each channel\n",
        "alpha = jnp.linspace(0.1, 0.8, n_channels)\n",
        "print(f\"\\nAdstock alpha (shape {alpha.shape}):\\n{alpha}\")\n",
        "print(f\"Adstock max_lag: {max_lag}\")\n",
        "print(f\"Adstock n_times_output: {n_times_output}\")\n",
        "\n",
        "# --- Hill Parameters ---\n",
        "# Example EC50 values (half-saturation point) for each channel\n",
        "# Scaled by a factor, assuming media data represents impressions/spend\n",
        "ec = jnp.linspace(0.1, 0.5, n_channels) * 1e5 \n",
        "# Example slope values for each channel\n",
        "slope = jnp.linspace(1.0, 3.0, n_channels)\n",
        "print(f\"\\nHill EC50 (shape {ec.shape}):\\n{ec}\")\n",
        "print(f\"Hill slope (shape {slope.shape}):\\n{slope}\")"
       ]
    },
    {
       "cell_type": "markdown",
       "metadata": {
         "id": "demo_adstock_markdown"
        },
       "source": [
        "Apply the `adstock_jax` function to the raw media data."
       ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "call_adstock_jax"
      },
      "outputs": [],
      "source": [
        "adstocked_media = adstock_jax(media_data_jax, alpha, max_lag, n_times_output)\n",
        "print(\"Shape of adstocked_media:\", adstocked_media.shape)"
       ]
    },
    {
       "cell_type": "markdown",
       "metadata": {
         "id": "demo_hill_markdown"
        },
       "source": [
        "Next, apply the `hill_jax` function to the *output* of the adstock transformation (`adstocked_media`) to model saturation effects."
       ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "call_hill_jax"
      },
      "outputs": [],
      "source": [
        "hill_media = hill_jax(adstocked_media, ec, slope)\n",
        "print(\"Shape of hill_media:\", hill_media.shape)"
       ]
    },
    {
       "cell_type": "markdown",
       "metadata": {
         "id": "demo_slice_markdown"
        },
       "source": [
        "Display a small slice of the final transformed data (first 2 geos, first 5 time points, first channel)."
       ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "show_output_slice"
      },
      "outputs": [],
      "source": [
        "print(\"Slice of hill_media[:2, :5, 0]:\\n\", hill_media[:2, :5, 0])"
       ]
    },
    {
       "cell_type": "markdown",
       "metadata": {
         "id": "extract_kpi_markdown"
        },
       "source": [
        "Before defining the model, we also need to extract the target Key Performance Indicator (KPI) data that the model will try to predict."
       ]
    },
    {
       "cell_type": "code",
       "execution_count": null,
       "metadata": {
         "id": "extract_kpi_code"
        },
       "outputs": [],
       "source": [
        "# Extract KPI data (e.g., conversions) as a JAX array\n",
        "# The loaded 'data' is an xarray Dataset. We access the 'kpi' DataArray.\n",
        "target_kpi_jax = jnp.asarray(data['kpi'].values)\n",
        "\n",
        "# Print the shape (expected: Time x Geo)\n",
        "print(\"Shape of target KPI data (Time x Geo):\", target_kpi_jax.shape)"
       ]
    },
    {
       "cell_type": "markdown",
       "metadata": {
         "id": "tfp_model_header_markdown"
        },
       "source": [
        "## Step 6: Define a Simple TFP-on-JAX Model"
       ]
    },
    {
       "cell_type": "markdown",
       "metadata": {
         "id": "tfp_model_explainer_markdown"
        },
       "source": [
        "Now, let's define a simple Media Mix Model using TFP-on-JAX's `JointDistributionCoroutine`. This model will incorporate the `adstock_jax` and `hill_jax` functions we defined earlier.\n",
        "\n",
        "For simplicity in this demonstration, we will:\n",
        "1. Use **fixed** parameters for the Adstock (`alpha`, `max_lag`) and Hill (`ec`, `slope`) transformations, using the example values defined in the previous section.\n",
        "2. Define **priors** only for the intercept, the media channel coefficients (`beta_media`), and the observation noise (`sigma`).\n",
        "3. Assume a simple linear relationship between the transformed media contributions and the mean predicted KPI (`mu`).\n",
        "4. Use a Normal distribution for the likelihood of observing the actual KPI data given the predicted mean `mu` and noise `sigma`."
       ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "define_tfp_model"
      },
      "outputs": [],
      "source": [
        "def create_mmm_model_fixed_transforms(media_data, fixed_alpha, fixed_ec, fixed_slope, fixed_max_lag, fixed_n_times_output):\n",
        "  \"\"\"Creates a TFP JointDistributionCoroutine MMM with fixed transforms.\"\"\"\n",
        "  \n",
        "  n_geos, n_times, n_channels = media_data.shape\n",
        "\n",
        "  @tfd.JointDistributionCoroutine\n",
        "  def model():\n",
        "    # --- Priors --- \n",
        "    # Noise level\n",
        "    sigma = yield tfd.HalfCauchy(loc=0., scale=5., name='sigma')\n",
        "    # Baseline KPI\n",
        "    intercept = yield tfd.Normal(loc=0., scale=5., name='intercept')\n",
        "    # Coefficients for each media channel's contribution\n",
        "    beta_media = yield tfd.Sample(tfd.Normal(loc=0., scale=1.), \n",
        "                                  sample_shape=[n_channels], \n",
        "                                  name='beta_media')\n",
        "    \n",
        "    # --- Transformations (using fixed parameters) --- \n",
        "    adstocked_media = adstock_jax(media_data, fixed_alpha, fixed_max_lag, fixed_n_times_output)\n",
        "    transformed_media = hill_jax(adstocked_media, fixed_ec, fixed_slope)\n",
        "    # transformed_media shape: [n_geos, n_times, n_channels]\n",
        "    \n",
        "    # --- Mean prediction (mu) --- \n",
        "    # intercept shape: [] (scalar)\n",
        "    # beta_media shape: [n_channels]\n",
        "    # Broadcasting: \n",
        "    # transformed_media * beta_media -> [n_geos, n_times, n_channels]\n",
        "    # jnp.sum(..., axis=-1) -> [n_geos, n_times]\n",
        "    # intercept + ... -> [n_geos, n_times]\n",
        "    mu = intercept + jnp.sum(transformed_media * beta_media, axis=-1)\n",
        "\n",
        "    # --- Likelihood --- \n",
        "    # sigma shape: [] (scalar)\n",
        "    # Need to expand sigma to match mu shape for Normal distribution\n",
        "    # Likelihood for KPI given mu and sigma\n",
        "    # Use Independent to declare Geo and Time as batch dimensions\n",
        "    kpi = yield tfd.Independent(tfd.Normal(loc=mu, scale=jnp.expand_dims(sigma, axis=(-1, -2))), \n",
        "                              reinterpreted_batch_ndims=2, \n",
        "                              name='kpi')\n",
        "  \n",
        "  return model\n",
        "\n",
        "# --- Instantiate the model --- \n",
        "# Use the JAX media data and the fixed parameters defined earlier\n",
        "mmm_model_fixed_jax = create_mmm_model_fixed_transforms(\n",
        "    media_data=media_data_jax,\n",
        "    fixed_alpha=alpha, # From cell 'define_demo_params'\n",
        "    fixed_ec=ec,       # From cell 'define_demo_params'\n",
        "    fixed_slope=slope, # From cell 'define_demo_params'\n",
        "    fixed_max_lag=max_lag, # From cell 'define_demo_params'\n",
        "    fixed_n_times_output=n_times_output # From cell 'define_demo_params'\n",
        ")\n",
        "\n",
        "print(\"Instantiated TFP-on-JAX Model (fixed transforms):\\n\", mmm_model_fixed_jax)"
       ]
    },
    {
       "cell_type": "markdown",
       "metadata": {
         "id": "prior_pred_header_markdown"
        },
       "source": [
        "### Step 6.1: Prior Predictive Sampling"
       ]
    },
    {
       "cell_type": "markdown",
       "metadata": {
         "id": "prior_pred_explainer_markdown"
        },
       "source": [
        "Before fitting the model to the actual data (which involves MCMC sampling from the posterior), it's crucial to perform a **prior predictive check**. This involves sampling from the model using only the prior distributions.\n",
        "\n",
        "The goal is to see if the priors we've defined generate plausible data *before* we let the model see the real `target_kpi_jax`. If the prior predictive samples look completely unreasonable (e.g., predicting wildly different scales, shapes, or distributions than expected for KPI), it suggests our priors might be poorly chosen or the model structure might be misspecified. For example, we might expect KPI values to be generally positive and unimodal.\n",
        "\n",
        "We will:\n",
        "1. Sample from the `mmm_model_fixed_jax` using its `.sample()` method.\n",
        "2. Extract the sampled KPIs (`kpi` variable from the model).\n",
        "3. Visualize the distribution of these prior predictive KPIs for a specific data point."
       ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "prior_pred_sample_code"
      },
      "outputs": [],
      "source": [
        "# Import plotting library\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set number of prior samples\n",
        "num_prior_samples = 500\n",
        "\n",
        "# Create a JAX PRNG key\n",
        "key = jax.random.PRNGKey(123)\n",
        "\n",
        "# Sample from the model (prior predictive)\n",
        "# For JDCoroutine, use value=None to sample all variables defined in the model\n",
        "prior_samples = mmm_model_fixed_jax.sample(value=None, seed=key, sample_shape=num_prior_samples)\n",
        "\n",
        "# Extract the prior predictive samples for the KPI\n",
        "prior_kpi_samples = prior_samples['kpi']\n",
        "\n",
        "# Print the shape of the prior predictive KPI samples\n",
        "# Expected shape: [num_prior_samples, n_geos, n_times]\n",
        "print(\"Shape of prior predictive KPI samples:\", prior_kpi_samples.shape)"
       ]
    },
    {
       "cell_type": "markdown",
       "metadata": {
         "id": "prior_pred_viz_markdown"
        },
       "source": [
        "Let's visualize the distribution of these prior predictive KPI samples for a single geo (index 0) at a single time point (index 10).\n",
        "\n",
        "Check the histogram: Does the range of predicted KPI values seem plausible based on domain knowledge? Is the shape roughly what you might expect, or does it reveal potential issues with the priors (e.g., unintended multi-modality)?"
       ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "prior_pred_viz_code"
      },
      "outputs": [],
      "source": [
        "# Select samples for Geo 0, Time 10\n",
        "geo_idx = 0\n",
        "time_idx = 10\n",
        "samples_point = prior_kpi_samples[:, geo_idx, time_idx]\n",
        "\n",
        "# Plot histogram\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.hist(samples_point, bins=30, density=True)\n",
        "plt.title(f'Prior Predictive Distribution for KPI (Geo {geo_idx}, Time {time_idx})')\n",
        "plt.xlabel('Predicted KPI Value')\n",
        "plt.ylabel('Density')\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "plt.show()\n",
        "\n",
        "# Also print basic stats\n",
        "print(f\"\\nPrior Predictive KPI Stats (Geo {geo_idx}, Time {time_idx}):\")\n",
        "print(f\"Mean: {jnp.mean(samples_point):.2f}\")\n",
        "print(f\"Std Dev: {jnp.std(samples_point):.2f}\")\n",
        "print(f\"Min: {jnp.min(samples_point):.2f}\")\n",
        "print(f\"Max: {jnp.max(samples_point):.2f}\")"
       ]
    },
    {
       "cell_type": "markdown",
       "metadata": {
         "id": "mcmc_header_markdown"
        },
       "source": [
        "### Step 6.2: Posterior Sampling (MCMC)"
       ]
    },
    {
       "cell_type": "markdown",
       "metadata": {
         "id": "mcmc_explainer_markdown"
        },
       "source": [
        "Now that we have defined the model and checked the priors, we can perform **posterior inference** using Markov Chain Monte Carlo (MCMC). The goal is to sample from the posterior distribution `p(parameters | observed_kpi)`, which tells us the plausible values for our parameters (`sigma`, `intercept`, `beta_media`) given the data.\n",
        "\n",
        "We need:\n",
        "1.  **Target Log Probability Function:** A function that takes the parameters we want to sample (`sigma`, `intercept`, `beta_media`) as input and returns the log probability of the model *given the observed KPI data*. This function effectively calculates `log p(parameters) + log p(observed_kpi | parameters)` by calling `model.log_prob()` and supplying (or \"pinning\") the actual `target_kpi_jax` to the `kpi` argument of `log_prob`.\n",
        "2.  **MCMC Sampler:** We'll use the No-U-Turn Sampler (NUTS), specifically the `tfp.experimental.mcmc.windowed_adaptive_nuts` kernel. NUTS is an efficient gradient-based MCMC algorithm. The `windowed_adaptive` variant automatically tunes the sampler's step size during an initial adaptation/burn-in phase.\n",
        "3.  **Initial State:** Starting values for the parameters (`sigma`, `intercept`, `beta_media`). We often initialize multiple chains starting from different points, typically drawn from the prior distribution, to help assess convergence.\n",
        "4.  **Sampling Loop:** Run the NUTS kernel for a specified number of steps. This typically includes:\n",
        "    *   *Burn-in (or Warmup/Adaptation):* An initial period where the sampler adapts and converges towards the high-density region of the posterior. These samples are usually discarded.\n",
        "    *   *Sampling (or Results):* The period after burn-in where we collect samples to approximate the posterior distribution.\n",
        "5.  **JIT Compilation:** Use `@jax.jit` to compile the MCMC sampling loop. This is crucial for performance, especially with complex models or large datasets, as it optimizes the computation graph.\n",
        "\nThe output `states` from `tfp.experimental.mcmc.run_kernel` (when using NUTS on a target function with multiple inputs) will be a list where each element corresponds to the samples for one input parameter of the `target_log_prob_fn`, ordered positionally."
       ]
    },
    {
       "cell_type": "code",
       "execution_count": null,
       "metadata": {
         "id": "mcmc_target_log_prob_code"
        },
       "outputs": [],
       "source": [
        "# Define the target log probability function\n",
        "# This function takes parameters as input and returns the model's log probability\n",
        "# evaluated with the *observed* KPI data pinned.\n",
        "def target_log_prob_fn(sigma, intercept, beta_media):\n",
        "  return mmm_model_fixed_jax.log_prob({\n",
        "      'sigma': sigma,\n",
        "      'intercept': intercept,\n",
        "      'beta_media': beta_media,\n",
        "      'kpi': target_kpi_jax # Pin observed data here!\n",
        "  })\n",
        "\n",
        "# Example: Evaluate log prob at a point (e.g., prior sample)\n",
        "example_params = mmm_model_fixed_jax.sample(value=None, seed=jax.random.PRNGKey(0), sample_shape=()) \n",
        "example_log_prob = target_log_prob_fn(**{k: example_params[k] for k in ['sigma', 'intercept', 'beta_media']})\n",
        "print(f\"Example Log Prob at prior sample: {example_log_prob}\")"
       ]
    },
    {
       "cell_type": "code",
       "execution_count": null,
       "metadata": {
         "id": "mcmc_run_code",
         "tags": []
        },
       "outputs": [],
       "source": [
        "# Import MCMC utilities\n",
        "from tfp.experimental import mcmc\n",
        "import time\n",
        "\n",
        "# MCMC settings\n",
        "num_burnin_steps = 500\n",
        "num_results = 1000\n",
        "num_chains = 4 # For parallel chains\n",
        "\n",
        "# --- Initial state --- \n",
        "# Get initial states by sampling from the prior (one for each chain)\n",
        "key_init, key_mcmc = jax.random.split(jax.random.PRNGKey(42))\n",
        "initial_state_params = mmm_model_fixed_jax.sample(value=None, seed=key_init, sample_shape=[num_chains])\n",
        "# Keep only the parameters we are sampling (sigma, intercept, beta_media)\n",
        "initial_state = [\n",
        "    initial_state_params['sigma'],\n",
        "    initial_state_params['intercept'],\n",
        "    initial_state_params['beta_media']\n",
        "]\n",
        "print(f\"Initial state shapes: {[s.shape for s in initial_state]}\")\n",
        "\n",
        "# --- MCMC Kernel --- \n",
        "# Create the Windowed Adaptive NUTS kernel\n",
        "#Adaptation steps should be a fraction of burn-in\n",
        "num_adaptation_steps = int(num_burnin_steps * 0.8)\n",
        "adaptive_kernel = mcmc.windowed_adaptive_nuts(\n",
        "    target_log_prob_fn=target_log_prob_fn, \n",
        "    num_adaptation_steps=num_adaptation_steps\n",
        ") \n",
        "\n",
        "# --- JIT-compiled MCMC runner --- \n",
        "@jax.jit\n",
        "def run_mcmc_chain(key, initial_state):\n",
        "  # Run the kernel\n",
        "  # `run_kernel` returns (states, kernel_results)\n",
        "  states, kernel_results = mcmc.run_kernel(\n",
        "      kernel=adaptive_kernel,\n",
        "      num_steps=num_burnin_steps + num_results,\n",
        "      current_state=initial_state,\n",
        "      seed=key\n",
        "  )\n",
        "  return states, kernel_results\n",
        "\n",
        "# --- Run MCMC --- \n",
        "print(f\"Running {num_chains} NUTS chains for {num_burnin_steps} burn-in + {num_results} results...\")\n",
        "start_time = time.time()\n",
        "# Use vmap to run chains in parallel\n",
        "keys_mcmc = jax.random.split(key_mcmc, num_chains)\n",
        "states, kernel_results = jax.vmap(run_mcmc_chain)(keys_mcmc, initial_state)\n",
        "end_time = time.time()\n",
        "print(f\"MCMC finished in {end_time - start_time:.2f} seconds.\")\n",
        "\n",
        "# states is a list [sigma_samples, intercept_samples, beta_media_samples]\n",
        "# Each element has shape [num_chains, num_steps, ...parameter_dims...]\n",
        "print(f\"\\nRaw output shapes (chains, steps, ...): {[s.shape for s in states]}\")"
       ]
    },
        {
       "cell_type": "markdown",
       "metadata": {
         "id": "mcmc_postprocess_markdown"
        },
       "source": [
        "Extract the posterior samples after discarding the burn-in steps."
       ]
    },
    {
       "cell_type": "code",
       "execution_count": null,
       "metadata": {
         "id": "mcmc_postprocess_code"
        },
       "outputs": [],
       "source": [
        "# Discard burn-in steps and combine chains\n",
        "# states is a list: [sigma_samples, intercept_samples, beta_media_samples]\n",
        "# Each item has shape [num_chains, num_steps, ...]\n",
        "# We want shape [num_chains * num_results, ...]\n",
        "\n",
        "posterior_samples = {\n",
        "    'sigma': states[0][:, num_burnin_steps:].reshape(-1, *states[0].shape[2:]),\n",
        "    'intercept': states[1][:, num_burnin_steps:].reshape(-1, *states[1].shape[2:]),\n",
        "    'beta_media': states[2][:, num_burnin_steps:].reshape(-1, *states[2].shape[2:]),\n",
        "}\n",
        "\n",
        "print(\"Posterior sample shapes (merged_samples, ...):\")\n",
        "for name, samples in posterior_samples.items():\n",
        "  print(f\"  {name}: {samples.shape}\")"
       ]
    },
    {
       "cell_type": "markdown",
       "metadata": {
         "id": "mcmc_analysis_header_markdown"
        },
       "source": [
        "### Step 6.3: Analyze MCMC Results"
       ]
    },
    {
       "cell_type": "markdown",
       "metadata": {
         "id": "mcmc_analysis_explainer_markdown"
        },
       "source": [
        "After running MCMC, we need to analyze the results to understand the posterior distribution and check if the sampler behaved well.\n",
        "\n",
        "Key checks include:\n",
        "1.  **Summary Statistics:** Calculate metrics like mean, median, and standard deviation for each parameter's posterior samples. This gives us point estimates (like the posterior mean) and a measure of uncertainty (like the posterior standard deviation).\n",
        "2.  **Trace Plots:** Visualize the sampled values for each parameter across all MCMC steps (including burn-in) for each chain. Well-behaved chains should:\n",
        "    *   Look like stationary \"fuzzy caterpillars\" after the burn-in period, indicating the sampler is exploring a stable distribution.\n",
        "    *   Show good mixing, meaning the chains explore the parameter space effectively without getting stuck.\n",
        "    *   Have different chains overlapping in the same region, suggesting they all converged to the same posterior distribution.\n",
        "3.  **Diagnostics:**\n",
        "    *   **R-hat (Potential Scale Reduction Factor):** Compares the variance within each chain to the variance between chains. Values close to 1.0 (ideally **< 1.05**, sometimes relaxed to < 1.1) suggest that all chains have converged to the same distribution.\n",
        "    *   **Effective Sample Size (ESS):** Estimates the number of *effectively independent* samples obtained, accounting for autocorrelation within the chains. Higher ESS values are better, indicating more efficient exploration of the posterior and more reliable estimates. Low ESS might suggest needing longer sampling runs or sampler tuning."
       ]
    },
    {
       "cell_type": "code",
       "execution_count": null,
       "metadata": {
         "id": "mcmc_summary_stats_code"
        },
       "outputs": [],
       "source": [
        "# Calculate and print summary statistics for posterior samples\n",
        "print(\"Posterior Summary Statistics:\")\n",
        "for name, samples in posterior_samples.items():\n",
        "  print(f\"\\nParameter: {name}\")\n",
        "  print(f\"  Mean:   {jnp.mean(samples, axis=0)}\")\n",
        "  print(f\"  Median: {jnp.median(samples, axis=0)}\")\n",
        "  print(f\"  Std Dev:{jnp.std(samples, axis=0)}\")"
       ]
    },
    {
       "cell_type": "code",
       "execution_count": null,
       "metadata": {
         "id": "mcmc_trace_plot_code"
        },
       "outputs": [],
       "source": [
        "# Plot trace plots for key parameters\n",
        "# Note: We use the 'states' variable which contains samples *before* burn-in removal and chain merging\n",
        "# states = [sigma_samples, intercept_samples, beta_media_samples]\n",
        "# Shapes: [num_chains, num_steps, ...]\n",
        "\n",
        "param_names = ['sigma', 'intercept', 'beta_media']\n",
        "num_params_to_plot = 3 # Plot sigma, intercept, and beta_media[0]\n",
        "\n",
        "fig, axes = plt.subplots(num_params_to_plot, 1, figsize=(10, 2 * num_params_to_plot), sharex=True)\n",
        "\n",
        "for i in range(num_params_to_plot):\n",
        "    param_idx = i\n",
        "    param_name = param_names[param_idx]\n",
        "    # Select the correct samples from the 'states' list\n",
        "    samples_all_chains = states[param_idx]\n",
        "    # If beta_media, select the first component\n",
        "    if param_name == 'beta_media':\n",
        "        samples_all_chains = samples_all_chains[:, :, 0]\n",
        "        plot_title = f'Trace Plot: {param_name}[0]'\n",
        "    else:\n",
        "        plot_title = f'Trace Plot: {param_name}'\n",
        "        \n",
        "    ax = axes[i]\n",
        "    # Plot samples for each chain\n",
        "    for chain_idx in range(num_chains):\n",
        "        ax.plot(samples_all_chains[chain_idx], alpha=0.7)\n",
        "    # Add vertical line for burn-in\n",
        "    ax.axvline(num_burnin_steps, color='red', linestyle='--', label=f'Burn-in ({num_burnin_steps} steps)')\n",
        "    ax.set_title(plot_title)\n",
        "    ax.set_ylabel('Sample Value')\n",
        "    ax.legend()\n",
        "    ax.grid(True, linestyle='--', alpha=0.6)\n",
        "\n",
        "axes[-1].set_xlabel('MCMC Step')\n",
        "fig.tight_layout()\n",
        "plt.show()"
       ]
    },
    {
       "cell_type": "code",
       "execution_count": null,
       "metadata": {
         "id": "mcmc_diagnostics_code"
        },
       "outputs": [],
       "source": [
        "# Calculate MCMC diagnostics (R-hat, ESS)\n",
        "from tfp import mcmc\n",
        "\n",
        "# Use samples *after* burn-in but *before* merging chains\n",
        "# states_after_burnin shape: [param_list, num_chains, num_results, ...]\n",
        "states_after_burnin = [\n",
        "    s[:, num_burnin_steps:] for s in states\n",
        "]\n",
        "\n",
        "print(\"MCMC Diagnostics (calculated on post-burn-in samples):\")\n",
        "for i, name in enumerate(param_names):\n",
        "    samples = states_after_burnin[i]\n",
        "    try:\n",
        "        # R-hat (Potential Scale Reduction)\n",
        "        rhat = mcmc.potential_scale_reduction(samples)\n",
        "        # ESS (Effective Sample Size)\n",
        "        ess = mcmc.effective_sample_size(samples)\n",
        "        \n",
        "        print(f\"\\nParameter: {name}\")\n",
        "        print(f\"  R-hat: {rhat}\")\n",
        "        print(f\"  ESS:   {ess}\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\nCould not compute diagnostics for {name}: {e}\")\n",
        "        # This might happen if shapes are unexpected or samples have issues (e.g., all NaNs)\n"
       ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
